---
layout: review
title: "MedTrinity-25M"
tags: dataset transformer medical
author: "Pierre-Marc Jodoin"
cite:
    authors: "Yunfei Xie, Ce Zhou1, Lang Gao1, Juncheng Wu2, Xianhang Li, Hong-Yu Zhou, Sheng Liu, Lei Xing, James Zou, Cihang Xie, Yuyin Zhou"
    title:   "MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"
    venue:   "arXiv:2408.02900"
pdf: "https://arxiv.org/pdf/2408.02900.pdf"
---


# Highlights

This paper introduces the largest dataset thus far designed to support multimodal medical AI tasks. It covers over **25 million** images across **10 modalities**, with detailed annotations for more than **65 diseases**. The dataset includes global textual information like disease type, modality, and region-specific descriptions, as well as local annotations for regions of interest (ROIs) such as bounding boxes and segmentation masks.

Key features of MedTrinity-25M include:

* *Automated Data Construction*: The dataset is built using an automated pipeline that scales up multimodal data by generating multigranular annotations from unpaired images without relying on text descriptions.
* *Dataset Composition*: Data from over 90 sources has been collected and preprocessed, with expert models used to identify ROIs related to abnormal regions. The data includes multigranular visual and textual annotations.
* *Applications*: MedTrinity-25M has been shown very effective a visual Q&A.

The most compelling parts of this paper is (1) how the dataset was created, curated and automatically annotated and (2) how it sparked  multimodal large language models (MLLM) onto having SOTA results on three VQ&A datasets. 

# Dataset creation and automcatic annotation

The MedTrinity-25M dataset is made of **triplets** i.e. ( **Image**, **Region of Interest (ROI)**, **Description**)

The way the dataset was created is illustrated in Figure 2.

![](/article/images/medtrinity/sc01.jpeg)


The **Automated Data Construction** process is designed to generate large-scale, multigranular annotations for medical images. The process involves several key steps:

#### 1. Data Collection and Preprocessing
- The dataset was assembled from over 90 online resources, including platforms like TCIA, Kaggle, Zenodo, and Synapse.
- Data collected included medical images with various levels of existing annotations, such as segmentation masks, lesion bounding boxes, or disease types, but often lacking detailed textual descriptions.
- Preprocessing steps included:
  - **Identification of Regions of Interest (ROIs)**: Using domain-specific expert models to locate abnormalities within the images.
  - **Metadata Integration**: Extracting and integrating metadata to generate coarse captions that provide fundamental information about each image, including modality, organ labels, and disease types.

#### 2. Generation of Multigranular Annotations
 The core of the automated pipeline involves using **Multimodal Large Language Models (MLLMs)** to generate detailed visual and textual annotations without the need of an expert.  With the use of a prompt template, the MLLM is prompted with **Annotations** include **Global Information** such as disease/lesion type, modality, and inter-regional relationships, an **Local Information** including detailed descriptions for ROIs, including bounding boxes, segmentation masks, and specific textual descriptions.  These annotations help create comprehensive image-ROI-description triplets.


#### 3. Quality 
- **Validation and Evaluation**: The generated multigranular descriptions are validated against human annotations to assess their accuracy and alignment. This involves comparing the structured descriptions generated by the MLLMs with human-generated text to ensure the descriptions are both accurate and comprehensive.


#### 4. Knowledge Retrieval and Annotation Refinement
- To ensure enriched and accurate annotations, the process incorporates **Retrieval-Augmented Generation (RAG)** techniques.
- A comprehensive knowledge base was built from medical literature, such as PubMed, to provide context and enhance the generated annotations.
- This step helps in standardizing medical terminology and refining diagnoses in the generated textual descriptions.

An example of this automatic annotation is illustrated here :

![](/article/images/medtrinity/sc02.jpeg)

# VQ&A

In order to underline the power of their dataset, the authors implemented a multimodal vision LLM called Large Language and Vision Assistant (**LLaVA**)[^1].
The main idea behing LLaVA implies that the LLM is fed with both image-based tokens and prompt tokens.


![](/article/images/medtrinity/sc03.jpeg)


# References

[^1]: [H Liu, C Li, Q Wu, YJ Lee Visual Instruction Tuning, NeurIPS 2023](https://llava-vl.github.io/)

