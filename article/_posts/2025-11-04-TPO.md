---
layout: review
title: "Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback"
tags: LLM, Preference optimization
author: "Arnaud Judge"
cite:
    authors: "Yafu Li, Xuyang Hu, Xiaoye Qu, Linjie Li, Yu Cheng"
    title:   "Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback"
    venue:   "ICML 2025"
pdf: "https://arxiv.org/abs/2501.12895"
---

# Test-Time Preference Optimization

TPO is a novel method for LLM alignment and preference optimization, which forgoes numerical rewards and more 
parameter optimization, replacing them with textual rewards and *textual gradients*.

By interacting with itself and a reward model, a LLM can optimize its context and find an optimal contextual parameter
that re-allocates probability mass with the model weights fixed. It can be viewed as an online, on-policy learning paradigm.

![](/article/images/TPO/summary.png)

# Background

Many preference optimization algorithms have been introduced in recent years. 
The goal is to increase likelihood of generating preferred outputs and decrease likelihood for misaligned outputs: 
$\textup{Max}_{\theta} \mathbb{E}_{(x, y_w, y_l) \sim D} [s(x, y_w, y_l)]$. 
This is done by updating the model's parameters with gradient descent.

They can be divided into 2 main categories:

1. Point-wise: Traditional RLHF methods such as PPO, which operate with many individual data points.

$$s(x, y) = r_{\phi}(x, y) - \beta \ KL(\pi_\theta(y|x) || \pi_{ref}(y|x))$$

2. Pair-wise: Methods such as DPO leverage comparisons between pairs of samples (accepted and rejected), to capture relative preferences.

$$s(x, y_w, y_l) = log \sigma (\beta  \textup{Log} \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)}) - \beta \textup{Log} \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)})$$


## Inference-time alignment:

Other methods can be used at inference, including in-context learning and retrieval-augmentation.
Best-of-N (BoN) methods that use a reward model to select the best outputs from a list of candidates generated by the policy model can also be used to improve model alignment.

# Method

Rather than finding optimal model parameters $\theta$, TPO searches for an optimal contextual parameter $\phi$ to modify the model's output distribution $p(y | \phi; \theta,x)$ with fixed $\theta$.
Instead of numerical gradient optimization, TPO operates entirely on textual variables (gradient, losses, etc.).

### Components
With $x$ as a query, $M$ as the LLM and $P$ as a textual prompt function (that contains instructions), TPO can be divided into 4 key components:
1. Variable definition (ie. model response): $v \leftarrow M(x)$
2. Loss calculation, with $P_{loss}$, expresses the loss function (preference heuristics): $L(x,y) = M(P_{loss}(x, v))$.
3. Gradient computation, with $P_{grad}$, integrates textual loss to obtain update instructions: $\frac{\delta L}{\delta v} \leftarrow M(P_{grad}(L(x, v)))$
4. Variable optimization, with $P_{update}$, generates a refined variable: $v_{new} \leftarrow M(P_{update}(\frac{\delta L}{\delta v}))$ 

### Alignment

For alignment, a reward model $R$ is used as a proxy for human preferences. Steps are the following:

1. First, $N$ candidate responses are sampled from $M$, and each response is evaluated with $R$. This is stored in a cache.
Based on reward scores a best and worst response is chosen.

2. The textual loss is computed with $M$, by prompting it with $P_{loss}$ and both responses: $L(x,y) = M(P_{loss}(x, v, \hat{v}))$ 

3. A textual gradient is derived from the textual loss and prompt $P_{grad}$. The gradient is a textual instruction to be given to the model. 
With this instruction, it generates new candidate responses which are added to the cache.  

After $D$ iterations of this procedure, the highest score response in the cache is selected as the final output.

Authors say the key insight behind TPO is: "to harness the LLMâ€™s innate instruction-following and reasoning capabilities for interpreting reward-model feedback and executing critiques".

![](/article/images/TPO/close_up_method.png)

# Results and Analysis

### Experimental Setup
Two policy categories are considered: unaligned and aligned. Aligned models have undergone preference optimization already in the form of RLHF or DPO.
Many models are tested for each type with variable sizes.
For rewards, a `FsfaireX-LLaMA3-RM-v0.1` model is used everywhere.

### Implementation
TextGrad (Yuksekgonul et al., 2024) is used for $P_{grad}$ and $P_{update}$ prompts while the $P_{loss}$ is customized for preference optimization.

## Results
Tests were lead on various benchmarks to evaluate TPO against the two baseline model types (instruction following, general preference alignment, safety and Math).
![](/article/images/TPO/main_results.png)

With only two steps, TPO brings unaligned model's performances at or beyond those of aligned models, for a fraction of the computational cost (roughly 0.01% of the PFLOPs).
Without leveraging rejected responses (revision), results are not as strong.
![](/article/images/TPO/optim_curves.png)

Stability of output responses is enhanced with TPO as well, meaning that the model outputs more deterministic responses (across response sampling) that are more often aligned with preferences.
![](/article/images/TPO/inference_stability.png)

One caveat is that smaller models (ex.: 8b) may not maintain alignment with TPO, possibly because they lack the ability to accurately follow instructions versus large models (70b). 

# Conclusion

TPO offers a new and interesting method to leverage the capabilities of a single LLMs in many different ways, simply by prompting it differently.

It resembles reasoning models, with help from a dedicated external reward models. Iterating over multiple responses.


# Appendix

Full example of TPO, from question to final response with loss and gradient:
![](/article/images/TPO/full_example.png)

More examples are available in the paper's Appendix D.