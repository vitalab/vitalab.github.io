---
layout: review
title: "Rethinking the Faster R-CNN Architecture for Temporal Action Localization"
tags: deep-learning CNN action-recognition
author: "Frédéric Branchaud-Charron"
cite:
    authors: "Yu-Wei Chao, Sudheendra Vijayanarasimhan, Bryan Seybold, David A. Ross, Jia Deng, Rahul Sukthankar"
    title:   "Rethinking the Faster R-CNN Architecture for Temporal Action Localization"
    venue:   "CVPR 2018"
pdf: "https://arxiv.org/pdf/1804.07667.pdf"
---

TAL-Net is an extension of Faster R-CNN to perform action recognition on sequences. This work proposes two main contributions:
* Variable receptive field
* Late feature fusion

## Variable receptive field
To better handle long sequences, they reuse the system of anchor but for different temporal spans.
They have $$K$$ 3D CNN for each temporal anchor. To get a matching dimension, they use a variable number of dilated 3D CNNs.
At the end, there is a classification task on which anchor to select.

![](/article/images/tal/fig3.png)

---

![](/article/images/tal/spn.png)

## Late feature fusion
Most action recognition techniques use the RGB image and the optical flow as their input.
Previous methods concatenated those streams where this method uses two distinct models where each model handles one stream before merging them.

![](/article/images/tal/fig6.png)


## Results

They test their method on THUMOS14 and test multiple temporal IoUs (tIoUs).
On tIoU = 0.5, they are better than the previous SotA by 11.8%.

![](/article/images/tal/table5.png)
