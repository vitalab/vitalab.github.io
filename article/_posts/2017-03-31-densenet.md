---
layout: review
title:  "DenseNet : Densely Connected Convolutional Networks"
tags:   deep-learning CNN classification essentials
author: Carl Lemaire
pdf:    https://arxiv.org/pdf/1608.06993.pdf
cite:
  authors: "Gao Huang, Zhuang Liu, Kilian Q Weinberger, Laurens van der Maaten"
  title:   "Densely connected convolutional networks"
  venue:   "arXiv preprint arXiv:1608.06993"
---

DenseNet is a CNN architecture that includes _dense blocks_. In a dense block, each layer has access to the feature maps of all preceding layers. Figure 1 shows a dense block.

![](/article/images/densenet/denseblock.png)

**Figure 1**: A simplified _dense block_ with two convolutional layers. The block takes $$ x_0 $$ as input and gives $$ [x_0,x_1,x_2] $$ as output. Here, each $$ x_i $$ is a set of three feature maps. We can see that $$ x_0 $$ is used to produce $$ x_1 $$, and **_reused_** to produce $$ x_2 $$.

DenseNets have many advantages :

* They require substantially fewer parameters and less computation to achieve state-of-the-art performances;
* They naturally integrate the properties of identity mappings and deep supervision;
* They are very compact, since they allow feature reuse.

## Architecture

Dense blocks are chained together using transition layers. The transition layers have the purpose of reducing the spatial extents of their inputs, and can also "compress" the feature maps to a smaller number. Figure 2 shows an example DenseNet architecture.

![](/article/images/densenet/fig2.png)

## Results

Different DenseNet architecture have stolen the state of the art from FractalNet and Wide ResNet for CIFAR-10, CIFAR-100 and Street View House Numbers. Moreover, DenseNets are on par with ResNets on the ImageNet classification task, using half the number of parameters (see section 4.4 for details).

![](/article/images/densenet/table2.png)

#### Efficiency on CIFAR-10

As you can see in the following figure, DenseNets have impressive parameter efficiency and computation efficiency. On the right, see the training and testing curves of a ResNet with **10.2M** parameters and a DenseNet with only **0.8M** parameters.

![](/article/images/densenet/fig4.png)

#### Efficiency on ImageNet

As with CIFAR-10, DenseNets are very efficient for the ImageNet classification task.

![](/article/images/densenet/fig3.png)

## Feature Reuse

A very important aspect of DenseNets is feature reuse inside dense blocks. To prove this, the authors made an experiment. The goal was to observe if the connections to early layers in a block are really important. After having trained a DenseNet, they have plotted (see below) the strength of the connections from a layer to all previous layers.

![](/article/images/densenet/fig5.png)

**Figure 5**: Strength of the connections from a layer $$ l $$ to a source layer $$ s $$. The first row represents the connections from all layers to the **input of the block**. The last column represents the connections from the _transition layer_ to **all layers in the block**.
