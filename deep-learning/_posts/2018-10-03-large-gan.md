---
layout: review
title:  "Large Scale GAN Training for High Fidelity Natural Image Synthesis"
tags:   deep-learning GAN
author: Frédéric Branchaud-Charron
pdf:    https://arxiv.org/pdf/1809.11096.pdf
cite:
  authors: "Andrew Brock, Jeff Donahue, Karen Simonyan"
  title:   "Large Scale GAN Training for High Fidelity Natural Image Synthesis"
  venue:   "arXiv:1809.11096"
---

As we know, GANs are really tricky to train and they often produces abnormal results. The authors propose to fix all of this with the following contributions:
* Use bigger and better model
* Better sampling from the latent space

![](/deep-learning/images/large-gan/fig1.png)

Unfortunately, it will be incredibly hard to reproduce because it requires :
* JFT-300M one of the biggest dataset ever made
* 128-512 TPU v3 cores


### Bigger models

They start with a default SA-GAN, they swap the BatchNorm with **class-conditional BatchNorm** and provide class information to the discriminator with **projection**. They also find that GANs work best with big batch size even if the model will collapse sooner. Finally, they boosted the number of channels everywhere by a factor of 2.

![](/deep-learning/images/large-gan/fig15.png)


### Better sampling
To avoid large values in the latent space, they used a truncated normal where values outside a range are resampled. This range can now act as a tradeoff between diversity and fidelity

They did find that some models wouldn't work with the truncated normal, so they added a new regularization to force *G* to be smooth. This regularizer aims to minimize the pairwise cosine similarity but does not constrain the norm of the filters.

![](/deep-learning/images/large-gan/eq3.png)

This is highly similar to the orthogonality regularizer.


---

![](/deep-learning/images/large-gan/fig13.png)
