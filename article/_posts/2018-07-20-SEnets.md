---
layout: review
title: Squeeze-and-Excitation Networks (2017 ImageNet winner)
tags: deep-learning CNN essentials
author: "Daniel JÃ¶rgens"
cite:
    authors: "Hu, J and Shen, L and Sun, G"
    title:   "Squeeze-and-Excitation Networks"
    venue:   "CVPR 2018"
pdf: "https://arxiv.org/pdf/1709.01507.pdf"
---

#### Code
*Caffe* implementation available at [https://github.com/hujie-frank/SENet](https://github.com/hujie-frank/SENet)


# Contribution

The authors introduce an extension called *'Squeeze-and-Excitation'* (SE) block which should enable a network
"to perform *feature recalibration* through which it can [...] selectively emphasise [...] and suppress" features. 

They show how such SE-blocks improve performance on several datasets for several architectures while
maintaining a reasonable network complexity (in terms of number of parameters as well as computational load).


# Proposed mechanism

The basic idea is to enforce the network to regard non-linear interdependencies between spatial features in different
channels without any supervised intervention. This is achieved by reducing the output features of a transform
block of the original network by a global statistic (e.g. global average pooling) and predicting a scalar
weight per channel from such a vector of channel-wise (scalar) statistics.

#### SE-block

<p style="text-align:center"><img src="/article/images/SEnets/se_block.png" width="700"></p>

 * $$\mathbf{F}_{tr}$$: *transform* of the original network, e.g. convolutional block
 * $$z_c = \mathbf{F}_{sq}(\mathbf{u}_c) = \frac{1}{H \times W} \sum_{i=1}^H \sum_{j=1}^W \, u_c(i, j)$$: *squeeze* operation
 * $$\mathbf{s} = \mathbf{F}_{ex}(\mathbf{z}, \mathbf{W}) = \sigma(g(\mathbf{z}, \mathbf{W}))
                = \sigma(\mathbf{W}_2\delta(\mathbf{W}_1\mathbf{z}))$$: *excitation* operation ($$\delta$$: ReLU)
 * $$\tilde{\mathbf{x}}_c = \mathbf{F}_{scale}(\mathbf{u}_c, s_c) = s_c \cdot \mathbf{u}_c$$: *recalibration* operation (i.e. rescaling)

#### Examples for extension of existing architectures

<p style="text-align:center">
  <img src="/article/images/SEnets/inception_module.png" width="330">
  <img src="/article/images/SEnets/resnet_module.png" width="330">
</p>


# Experiments

#### Extension of existing architectures

<p style="text-align:center"><img src="/article/images/SEnets/performance_table.png" width="700"></p>

 * SE blocks led to improvements for all investigated base-networks on the ImageNet 2012 dataset.
 * The computational overhead is small.
 * The improvement is the same for different network depths.
 
#### Different data sets

 * Similar improvements were also shown for other datasets
 
   * Scene Classification: *Places365*
   * Object Detection: *COCO*

#### Analysis of reweighting step

<p style="text-align:center"><img src="/article/images/SEnets/excitation_activations.png" width="700"></p>

**Caption:** Colored curves represent the average activations for different classes (computed over 50 samples for each class)
plotted over channel index.

 * In 'early' layers, the activations of the excitation step (i.e. rescaling weights) are the same among different classes.
 * In 'later' layers (e, f), the activations are saturated.
 * The reweighting among channels seems to be most significant in 'intermediate' layers.
