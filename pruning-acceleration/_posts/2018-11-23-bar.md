---
layout: review
title:  "Structured Pruning of Neural Networks with Budget-Aware Regularization"
tags:   deep-learning network-pruning network-compression
author: Carl Lemaire
pdf:    ""
cite:
    authors: "Carl Lemaire, Andrew Achkar, Pierre-Marc Jodoin"
    title:   "Structured Pruning of Neural Networks with Budget-Aware Regularization"
    venue:   ""
---

Budget-Aware Regularization (BAR) allows to simultaneously train and prune a neural network architecture, while respecting a neuron budget. The method is targeted at "turning off" unrelevant feature maps in CNNs. To find which feature maps are relevant, "Dropout Sparsity Learning" is used. To respect the budget, a novel barrier function is introduced.

# Dropout Sparsity Learning

We learn which feature maps to keep by using a special kind of dropout. The first difference with traditional dropout is that each feature map is associated with a different Bernouilli parametrization (e.g. probability of being active). Let's call this "feature-wise dropout".

![](/pruning-acceleration/images/bar/eq1.png)

$$\mathbf{z}$$ is a vector of "dropout variables". The goal is to learn the parametrization of each dropout variable, with a preference for parametrizations that turn off the feature map. However, ordinary feature-wise dropout, because of its discrete nature, does not allow to learn the parameters by backpropagation. We will thus use a continous relaxation of Bernouilli, named the Hard Concrete distribution. Using the Reparametrization Trick, we can draw samples $$z = g(\Phi_{l-1},\epsilon), z \in [0,1]$$ from this distribution.

![](/pruning-acceleration/images/bar/eq2.png)

# Budget-Aware Regularization

To promote the removal of neurons, a sparsity term is used in the loss. To enforce the budget, a barrier function is used:

![](/pruning-acceleration/images/bar/barrier.png)

where $$V$$ is the volume of the activation maps and $$b$$ is the budget. The barrier function is multiplied with the sparsity loss:

![](/pruning-acceleration/images/bar/Lbar.png)

The complete loss function is the sum of a Knowledge Distillation objective (optional, cross entropy can be used instead) and the sparsity loss.

# Results

At the highest pruning factors, this method outperforms the state of the art.

![](/pruning-acceleration/images/bar/results.png)