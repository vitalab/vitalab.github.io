---
layout: review
title:  "GMM-Based Synthetic Samples for Classification of Hyperspectral Images With Limited Training Data"
tags:   machine-learning hyperspectral, classification, remote-sensing, GMM, PCA,
author: Charles Authier
pdf:    https://arxiv.org/pdf/1712.04778v1
cite:
  authors: "AmirAbbas Davari, Erchan Aptoula, Berrin Yanikoglu, Andreas Maier, Christian Riess"
  title:   "GMM-Based Synthetic Samples for Classification of Hyperspectral Images With Limited Training Data"
  venue:   "Published 2017 in ArXiv"
---

### Description
A big issue with images classification is the limitation of training data. In hyperspectral, the data have high dimensional feature vector, that part can be useful. They propose to compensate this limitation by adding synthetic samples drawn from a Gaussian mixture that is estimated on the feature space.

<img src="/article/images/GMMHSRS/workflow.jpg" width="600">

### Related Work
- GANs in deep-learning.
- Robust Classification: SVM, second part.
- Dimensionality Reduction: PCA

### Evaluation
They use the k-means clustering algorithm to provide reasonable initial values of the estimator.

First reduction (PCA) preserve 99% of the total spectral variance. The result will pass in an extended multi-attribute profile (EMAP). They use four thresholds $$lambda$$: area, diagonal of the bounding box, the standard deviation of the gray values and the moment of inertia. All of those have different values possible.

For the second dimension reduction, non-parametric weighted feature extraction (NWFE) is used to preserve 99% of the feature variance.

For classification, they use a random forest, each experiment is repeated 25 times and the mean average accuracy (AA), overall accuracy (OA) and the Kappa along with their std are reported.

<img src="/article/images/GMMHSRS/performence.jpg" width="600">

### Compare
- Li et al. [^fn]: For 20, 30 and 40 training samples, OA is slightly higher, AA is comparable and for the Kappas, their approach is in all considerably higher.

- Aptoula et al. [^fn2]: Deep-learning classification (CNN), better results for all, but 6 to14 times more training samples used.

<img src="/article/images/GMMHSRS/cnnmetode.jpg" width="600">

- Tao et al. [^fn3]: Deep autoencoder, better Kappa, 0.9699 for 50 samples compared to 0.9528 for 40 in their case. Again, the other method higher number of samples.

### Results
- 13 pixels per class: [OA,AA] = [79.52, 84.78]
- 30 pixels per class: [OA,AA] = [76.06, 82.67]
- 40 pixels per class: [OA,AA] = [77.12, 85.12]

<img src="/article/images/GMMHSRS/GTcompare.jpg" width="600">

<img src="/article/images/GMMHSRS/table2.jpg" width="300"><img src="/article/images/GMMHSRS/table3.jpg" width="300">

<img src="/article/images/GMMHSRS/table1.jpg" width="300"><img src="/article/images/GMMHSRS/table8.jpg" width="300"><img src="/article/images/GMMHSRS/table9.jpg" width="300">


***

[^fn]: J. Li, X. Huang, P. Gamba, J. M. Bioucas-Dias, L. Zhang, J. A. Benediktsson, and A. Plaza, “Multiple feature learning for hyperspectral image classification,” IEEE Transactions on Geoscience and Remote Sensing, vol. 53, no. 3, pp. 1592–1606, 2015.
[^fn2]: E. Aptoula, M. C. Ozdemir, and B. Yanikoglu, “Deep learning with attribute profiles for hyperspectral image classification,” IEEE Geoscience and Remote Sensing Letters, 2016.
[^fn3]: C. Tao, H. Pan, Y. Li, and Z. Zou, “Unsupervised spectral-spatial feature
learning with stacked sparse autoencoder for hyperspectral imagery classification,” IEEE Geoscience and Remote Sensing Letters, vol. 12, no. 12, pp. 2438–2442, 2015.
