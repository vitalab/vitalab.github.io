---
layout: review
title: "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets"
tags: deep-learning GAN 
author: "Nathan Painchaud"
cite:
    authors: "Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, Pieter Abbeel"
    title:   "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets"
    venue:   "Advances in Neural Information Processing Systems, 2016"
pdf: "https://arxiv.org/pdf/1606.03657.pdf"
---


![](/article/images/infogan/sc.png)
(Image taken from [HERE](https://towardsdatascience.com/infogan-generative-adversarial-networks-part-iii-380c0c6712cd))

# Highlights
InfoGAN is an GAN which can learn a **disantangled** latent space distribution in an unsupervised manner.  

# Introduction
A basic GAN is illustrated with the graph in the previous figure but without the red boxes.  The main idea of InfoGAN is to add two little things to the basic GAN.

First and foremost, InfoGAN use an extra latent vector (called code): $$c=(c_0, c_1, ..., c_n)$$ that is appended to the original latent input vector $$z$$.  Doing this turns the original generator $$G(z)$$ into $$G(z,c)$$.   
In order to make sure the generator does not simply ignore $$c$$, InfoGAN adds to the loss function a **mutual information** term.  In this way, the loss goes from a normal GAN loss 

![](/article/images/infogan/sc01.png)

to an infoGAN loss

![](/article/images/infogan/sc02.png)

The point of the mutual information term is to make sure that the produced image depends on the input vector $$c$$.  The minus in from of $$\lambda$$ is there because $$G$$ aims at minimizing the loss and that $$I$$ is strictly positive and maximal when the generated images are totally dependent of $$c$$. 

Considering that the mutual information equation is given by

![](/article/images/infogan/sc03.png)

where $$H$$ is the entropy, the authors argue that this value is hard to compute because it depends on the posterior $$P(c\|x)$$ which is a priori unknown.  As a workaround, they use a proxy $$Q(c'\| x)$$, thanks to the **variational inference theory**.

![](/article/images/infogan/sc04.png)


This brings us to the second GAN improvement : by the very nature of deep neural nets, $$Q(c'\| x)$$ is approximated by a neural network appended to the discriminator network (c.f. top figure).  The resulting loss becomes :

![](/article/images/infogan/sc05.png)


where $$L_I$$ is $$Q$$'s loss (crossentropy of other).

# Results

Results are quite convincing with well disantangled distributions.

![](/article/images/infogan/sc06.png)


