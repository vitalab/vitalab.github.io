---
layout: review
title: "MemSAM: Taming Segment Anything Model for Echocardiography Video Segmentation"
tags: SAM, Echocardiography 
author: "Arnaud Judge"
cite:
    authors: "Xiaolong Deng, Huisi Wu, Runhao Zeng, Jing Qin"
    title:   "MemSAM: Taming Segment Anything Model for Echocardiography Video Segmentation"
    venue:   "CVPR2024"
pdf: "https://openaccess.thecvf.com/content/CVPR2024/papers/Deng_MemSAM_Taming_Segment_Anything_Model_for_Echocardiography_Video_Segmentation_CVPR_2024_paper.pdf"
---

# MemSAM

MemSAM is a new video segmentation model designed for echocardiography, built on top of the SAM architecture.
It is designed to address specific issues related to the application of SAM to echocardiography and video segmentation more broadly, versus image segmentation.

A space-time memory allows to model to prompt itself for all but the first frame in a video sequence and it's training framework allows it be trained with fewer annotations in a semi-supervised manner.

![](/article/images/MemSAM/memsam_fig2.png)

## Background

SAM has been reimplemented for medical image segmentation, but these methods rely on dense annotations and prompts, making them mostly unsuitable for medical video segmentation.

A few methods have been proposed to adapt the SAM architecture to video segmentation, but they mostly remain limited to natural images, as medical images such as echocardiography carry much more noise.
This noise is carried through intermediate features in the models, rendering them inefficient.
MemSAM builds upon ideas brought forward, but applies them more specifically to the challenges of medical image segmentation.

## Method 

MemSAM is built on top of the original SAM model (image encoder, prompt encoder and mask decoder).
The image embedding obtained from the image encoder is projected to the memory, which allows MemSAM to create a memory prompt for the mask decoder. 
The memory is updated after each frame is processed.

![](/article/images/MemSAM/memsam_fig3.png)

### Memory

![](/article/images/MemSAM/memsam_fig4.png)

#### Reading


#### Updating


#### Memory reinforcement



Authors show the effect of memory reinforcement on the attention that the model outputs. 
The attention is more localized and more effective for the segmentation task.
![](/article/images/MemSAM/memsam_fig7.png)

## Experiments

The authors use the pre-trained SAM weights and fine-tune with both the CAMUS and Echonet datasets.
Only the image encoder from SAM was trained, along with memory elements added in.

Results show that the method improves performances compared to other SAM based methods and more traditional architectures.
![](/article/images/MemSAM/memsam_results.png)

Qualitative results show that the model is more robust to blurry/unclear boundary regions compared ot other methods. 

### Semi-supervised vs fully-supervised

MemSAM allows for flexible use of labels when available. 
Loss is only calculated on frames for which a label is available, in this case, on end-systolic and end-diastolic frames.
For the CAMUS dataset, which contains full segmentations for half-cycle sequences, they show improvement versus ES/ED frame annotations only. 

## Interpretation and conclusion

Overall, MemSAM offers an interesting new adaptation of the SAM architecture to echocardiography video segmentation. 
It offers good performance with few annotations.

One limitation is the heavy reliance on the first frame of the video, which determines the quality of the memory and following frame segmentations.
Failure cases are shown, demonstrating the issue.
![](/article/images/MemSAM/memsam_fig9.png)



