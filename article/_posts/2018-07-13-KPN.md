---
layout: review
title: Burst Denoising with Kernel Prediction Networks
tags: deep-learning CNN
author: "Daniel JÃ¶rgens"
cite:
    authors: "Mildenhall, B and Barron, J T and Chen, J and Sharlet, D and Ng, R and Carroll, R"
    title:   "Burst Denoising with Kernel Prediction Networks"
    venue:   "CVPR 2018"
pdf: "https://arxiv.org/pdf/1712.02327.pdf"
---

# Goal

The presented approach aims at learning a set of filters for denoising bursts of images taken
by hand-held cameras (e.g. in smartphones).


# Contributions

 * Synthetic data creation from general purpose images simulating characteristics of real cameras.
 * Pixel-wise 3D kernel prediction for denoising of burst image sequence.
 * Adaptive, weighted ('annealing') loss.
 * Generalisation to multiple noise levels.


# Conceptual overview

<p style="text-align:center"><img src="/article/images/KPN/overview.png" width="750"></p>

#### Basic concept

 * **input**
 
   * set of series of _N_ images (burst)
   * one image of pixel-wise noise estimates

 * encoder-decoder structure
 
 * **output**
 
   * _N_ filters per pixel in input space (filter size: $$K$$ by $$K$$)
   * synthesis of output image at pixel $$p$$ by
   
     $$ \hat{Y}^p = \frac{1}{N} \sum_{i=1}^N \, <f_i^p, V^p(X_i)>, $$
     
     where $$f^p_i$$ denotes the learned filter at pixel $$p$$ in input image $$i$$.

#### Loss

 * *main* objective
 
   * $$ L^2 $$-term on gamma-corrected images
   * $$ L^1 $$-term on gradients of gamma-corrected images
 
     $$ \ell(\hat{Y}, Y^\ast) = \lambda_2 \, \left\lVert\Gamma(\hat{Y}) - \Gamma(Y^\ast)\right\rVert_2^2
                              + \lambda_1 \, \left\lVert\nabla\Gamma(\hat{Y}) - \nabla\Gamma(Y^\ast)\right\rVert_1^2 $$
 
 * *annealed* loss (This is the actual loss term!)
 
   * time dependent _individual image_ loss term
   * **idea**: steer training in the beginning to avoid convergence to local minima

     $$ \mathcal{L}(X; Y^\ast, t) = \ell\left(\frac{1}{N}\sum_{i=1}^N \, f_i(X_i), Y^\ast\right)
                                  + \beta\alpha^t \sum_{i=1}^N \, \ell(f_i(X_i), Y^\ast) $$

#### Synthetic data creation

 * The authors develop an approach to model several artifacts involved in creation of raw data from real camera sensors,
   including e.g.

     * explicit model for signal noise
 
       <p style="text-align:center"><img src="/article/images/KPN/noise_levels.png" width="500"></p>

     * simulated misalignment due to sensor movement


# Experiments

#### Settings

 * filter size: _K_ = 5
 * number of input images: _N_ = 8

#### Nomenclature (KPN methods)

 * _1-frame_: _N_ = 1
 * _no ann_: basic loss function only (@see *main* objective)
 * _sigma blind_: no noise estimate as input
 * _direct_: directly synthesise output pixel values (by adding three additional conv layers)

#### Synthetic data set

 * _KPN_ always outperforms state of the art
 * multi-frame info, annealing loss and noise estimate are all helpful

   <p style="text-align:center"><img src="/article/images/KPN/synthetic.png" width="500"></p>

#### Real data set

   <p style="text-align:center"><img src="/article/images/KPN/real_dataset.png" width="700"></p>

#### Predicted kernels

 * The approach is robust to object movement (the mouse).
 * The authors claim that their 'annealing' approach helps focusing on a single frame where movement occurs,
   while taking advantage of all frames in static parts of the image (i.e. background).

   <p style="text-align:center"><img src="/article/images/KPN/predicted_kernels.png" width="500"></p>

#### Adaptation to noise levels

 * Noise estimation
 
   * did not improve training loss
   * but helps for **generalisation** to larger (unseen) noise levels

 * Learned filters adapt to the _noise estimate_ in the input
 
   * (a) to (e): scalar multiples of _noise estimate_ for same image burst
 
   <p style="text-align:center"><img src="/article/images/KPN/noise_adaption.png" width="700"></p>
 
