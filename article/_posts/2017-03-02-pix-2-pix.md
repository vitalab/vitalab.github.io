---
layout: review
title:  "Image-to-Image Translation with Conditional Adversarial Networks"
author: "Frédéric Branchaud-Charron"
cite:
    authors: "Isola, P., Zhu, J. Y., Zhou, T., & Efros, A. A"
    title:   "Image-to-Image Translation with Conditional Adversarial Networks"
    venue:   "arXiv preprint arXiv:1611.07004"
pdf: "https://arxiv.org/pdf/1611.07004.pdf"
tags: cnn deep-learning gan generative
---

### Summary

Image-to-Image (or Pix-2-Pix) is a type of conditional GAN that does not use noise to generate its output. Instead, it uses a label map as an input to the generator. The discriminator receives the label map and the generated image to make its decision.

![](/article/images/pix-2-pix/pix-2-pix.png)

### Generator
The generator is basically an [U-Net]({{ site.baseurl }}{% link article/_posts/2017-02-27-unet.md %}) which takes the label map as the input and produces an image.

### Discriminator
The discriminator is a normal CNN with a sigmoid activation layer at the end. It also uses LeakyReLU for every convolutional layer. The input of the discriminator is both the label map and the generated image.

### Loss
Pix-2-Pix is using the standard conditional GAN loss plus a regularisation term L1.

![](/article/images/pix-2-pix/pix2pix_loss.png)

Where lambda is a hyperparameter (100 is suggested).

### Examples
![](/article/images/pix-2-pix/examples.jpg)

### Implementations
* [Pix2pix Caffe (Original Code)](https://github.com/phillipi/pix2pix)
* [Pix2pix Tensorflow](https://github.com/yenchenlin/pix2pix-tensorflow)
* [Pix2pix Keras](https://github.com/tdeboissiere/DeepLearningImplementations/tree/master/pix2pix)
