---
layout: review
title:  "PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning"
tags:   deep-learning network-pruning multi-task
author: Carl Lemaire
pdf:    https://arxiv.org/pdf/1711.05769.pdf
cite:
  authors: "Arun Mallya, Svetlana Lazebnik"
  title:   "PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning"
  venue:   "CVPR 2018"
---

# Background

The goal of this method is to solve multiple image classification tasks with a single network. Previous approches tried to avoid _catastrophic forgetting_ by using proxy losses. While training subsequent tasks, "Learning without Forgetting" (LwF) tried to preserve activations, and "Elastic Weight Consolidation" (EWC) tried to preserve the value of the weights.

# Approach

![](/article/images/packnet/fig1.jpg)

The suggested approach uses pruning to "pack" multiple models into one network:

1. Train task 1
2. Prune network and freeze sparsity pattern
3. Fine-tune task 1
4. Train task 2 with the "empty" neurons
5. Prune the neurons of task 2 (and freeze sparsity)
6. Fine-tune task 2
7. etc...

#### Pruning method

* Weight magnitude selection
* Select 50% or 75% of smallest weights for removal
* Unstructured sparsity

#### Inference

* Predict on a single task at once
* Apply task masks before inference
* Simultaneous inference requires filter-wise sparsity, they say, and performs worse

# Experimental results

They are better than previous methods, and only slightly worse than using an individual network per task (which is nice). They confirm that their method works with VGG16 (with Batch Norm), ResNet and DenseNet (see table 4 in paper).

![](/article/images/packnet/tab2.jpg)

![](/article/images/packnet/fig2.jpg)

![](/article/images/packnet/fig3.jpg)
