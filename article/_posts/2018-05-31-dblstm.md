---
layout: review
title: Densely Connected Bidirectional LSTM
tags: deep-learning CNN LSTM RNN
author: "Daniel JÃ¶rgens"
cite:
    authors: "Ding, Z and Xia, R and Yu, J and Li, X and Yang, J"
    title:   "Densely Connected Bidirectional LSTM with Applications to Sentence Classification"
    venue:   "arxiv.org"
pdf: "https://arxiv.org/pdf/1802.00889.pdf"
---

#### Code
available at [https://github.com/IsaacChanghau/Dense_BiLSTM](https://github.com/IsaacChanghau/Dense_BiLSTM)


# Contributions

 * Proposal of a densely-connected architecture applied to bi-directional LSTMs
 * Comparison against state of the art methods for sentence classification on five datasets


#### Advantages

The authors point out two main advantages of their approach

 * Easy trainability even of deep architectures.
 * Good parameter efficiency.


# Proposed model


#### Conceptual summary of architecture

<p style="text-align:center"><img src="/article/images/dblstm/architecture.png" width="500"></p>


#### Detailed comparison with *stacked RNN* architecture

<p style="text-align:center"><img src="/article/images/dblstm/architecture_detailed.png" width="1000"></p>


# Experiments

#### Data

**MR** - Movie review data; positive / negative <br>
**SST-1** - extension of *MR*; more fine-graned labels <br>
**SST-2** - *SST-1* with binary labels <br>
**Subj** - sentences; subjective / objective <br>
**TREC** - questions for 6 classes *person*, *location*, ...

<p style="text-align:center"><img src="/article/images/dblstm/data_summary.png" width="300"></p>

*c* - number of classes. <br>
*l* - average sentence length. <br>
*train* / *dev* / *test* - size of train / validation / test set ('*CV*' means 10-fold cross validation)


#### Main experiment

They achieve **state-of-the-art** performance superior to simple (Bi-)LSTM and CNN approaches.

<p style="text-align:center"><img src="/article/images/dblstm/result_acc.png" width="600"></p>


#### Further experiments

The authors conduct further experiments varying three of their hyperparameters:

 * *nb_last* units on last layer
 * *nb_layers*, i.e. the stacked LSTMs
 * *nb_hidden* units in all layers except the last


**Parameter efficiency**:
Increasing *nb_layers* while keeping number of parameters constant might improve accuracy.

**Increasing depth**:
Increasing *nb_layers* while keeping *nb_last* and *nb_hidden* constant improves accuracy.

**Increasing width**:
Increasing *nb_hidden* while keeping *nb_last* and *nb_hidden* constant improves accuracy.


**Comment** - In the last two settings also the number of parameters increases. Therefore, the improved
              accuracy could also be explained by that. The conclusions to the effects of *nb_layers*
              and *nb_hidden* are not ultimately convincing. 


# Comments

 * The authors claim that *'the application of DenseNet to RNN'* in NLP was novel.
   However, in [Godin (2017)](https://arxiv.org/pdf/1707.06130.pdf) the same idea was used for RNNs.